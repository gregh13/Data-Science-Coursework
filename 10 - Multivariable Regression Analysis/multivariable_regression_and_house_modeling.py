# -*- coding: utf-8 -*-
"""Multivariable_Regression_and_House_Modeling_Day_80

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1foz9c86etfYiNHxOyswvcZs3rMk7q-wk

<img src=https://i.imgur.com/WKQ0nH2.jpg height=350>

# Setup and Context

### Introduction

Welcome to Boston Massachusetts in the 1970s! Imagine you're working for a real estate development company. Your company wants to value any residential project before they start. You are tasked with building a model that can provide a price estimate based on a home's characteristics like:
* The number of rooms
* The distance to employment centres
* How rich or poor the area is
* How many students there are per teacher in local schools etc

<img src=https://i.imgur.com/WfUSSP7.png height=350>

To accomplish your task you will:

1. Analyse and explore the Boston house price data
2. Split your data for training and testing
3. Run a Multivariable Regression
4. Evaluate how your model's coefficients and residuals
5. Use data transformation to improve your model performance
6. Use your model to estimate a property price

### Upgrade plotly (only Google Colab Notebook)

Google Colab may not be running the latest version of plotly. If you're working in Google Colab, uncomment the line below, run the cell, and restart your notebook server.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade plotly

"""###  Import Statements

"""

import pandas as pd
import numpy as np

import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression

"""### Notebook Presentation"""

pd.options.display.float_format = '{:,.2f}'.format

"""# Load the Data

The first column in the .csv file just has the row numbers, so it will be used as the index. 
"""

# index_col since csv file already has index. Otherwise, get two index columns
data = pd.read_csv('boston.csv', index_col=0)

"""### Understand the Boston House Price Dataset

---------------------------

**Characteristics:**  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive. The Median Value (attribute 14) is the target.

    :Attribute Information (in order):
        1. CRIM     per capita crime rate by town
        2. ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        3. INDUS    proportion of non-retail business acres per town
        4. CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        5. NOX      nitric oxides concentration (parts per 10 million)
        6. RM       average number of rooms per dwelling
        7. AGE      proportion of owner-occupied units built prior to 1940
        8. DIS      weighted distances to five Boston employment centres
        9. RAD      index of accessibility to radial highways
        10. TAX      full-value property-tax rate per $10,000
        11. PTRATIO  pupil-teacher ratio by town
        12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        13. LSTAT    % lower status of the population
        14. PRICE     Median value of owner-occupied homes in $1000's
        
    :Missing Attribute Values: None

    :Creator: Harrison, D. and Rubinfeld, D.L.

This is a copy of [UCI ML housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/). This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. You can find the [original research paper here](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf?sequence=1&isAllowed=y).

# Preliminary Data Exploration üîé

**Challenge**

* What is the shape of `data`? 
* How many rows and columns does it have?
* What are the column names?
* Are there any NaN values or duplicates?
"""

display(data.shape)
display(data.sample(7))

data.count()

"""## Data Cleaning - Check for Missing Values and Duplicates"""

data.info()

display(data.isna().values.any())
display(data.duplicated().values.any())

"""## Descriptive Statistics

**Challenge**

* How many students are there per teacher on average?
* What is the average price of a home in the dataset?
* What is the `CHAS` feature? 
* What are the minimum and the maximum value of the `CHAS` and why?
* What is the maximum and the minimum number of rooms per dwelling in the dataset?
"""

display(data.describe())

"""Avg Pupil to Teacher: 18.46

Avg Price: $22.5k

Max/Min Price: $50k/$5k

Max/Min Rooms: 8.78/2.90

CHAS is boolean, 1 or 0

## Visualise the Features

**Challenge**: Having looked at some descriptive statistics, visualise the data for your model. Use [Seaborn's `.displot()`](https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot) to create a bar chart and superimpose the Kernel Density Estimate (KDE) for the following variables: 
* PRICE: The home price in thousands.
* RM: the average number of rooms per owner unit.
* DIS: the weighted distance to the 5 Boston employment centres i.e., the estimated length of the commute.
* RAD: the index of accessibility to highways. 

Try setting the `aspect` parameter to `2` for a better picture. 

What do you notice in the distributions of the data?

#### House Prices üí∞
"""

# kde=True superimposes the kde on the histogram.
with sns.axes_style("darkgrid"):
  ax1 = sns.displot(data, 
                    x='PRICE',
                    fill=True,
                    kde=True,
                    aspect=2)
plt.xlim(0, 60)
plt.show()

# Her Way:
# Put the descriptive mean data as title, nice!
sns.displot(data['PRICE'], 
            bins=50, 
            aspect=2,
            kde=True, 
            color='#2196f3')

plt.title(f'1970s Home Values in Boston. Average: ${(1000*data.PRICE.mean()):.6}')
plt.xlabel('Price in 000s')
plt.ylabel('Nr. of Homes')

plt.show()

"""#### Number of Rooms"""

with sns.axes_style("darkgrid"):
  ax1 = sns.displot(data, 
                    x='RM',
                    fill=True,
                    kde=True,
                    aspect=2)
plt.xlim(2, 10)
plt.show()

# Her Way:
sns.displot(data.RM, 
            aspect=2,
            kde=True, 
            color='#00796b')

plt.title(f'Distribution of Rooms in Boston. Average: {data.RM.mean():.2}')
plt.xlabel('Average Number of Rooms')
plt.ylabel('Nr. of Homes')

plt.show()

"""#### Distance to Employment - Length of Commute üöó"""

with sns.axes_style("darkgrid"):
  ax1 = sns.displot(data, 
                    x='DIS',
                    fill=True,
                    kde=True,
                    aspect=2)
# plt.xlim(0, 13)
plt.show()

"""#### Access to Highways üõ£"""

with sns.axes_style("darkgrid"):
  ax1 = sns.displot(data, 
                    x='RAD',
                    fill=True,
                    kde=True,
                    aspect=2)
# plt.xlim(0, 25)
plt.show()

"""#### Next to the River? ‚õµÔ∏è

**Challenge**

Create a bar chart with plotly for CHAS to show many more homes are away from the river versus next to it. The bar chart should look something like this:

<img src=https://i.imgur.com/AHwoQ6l.png height=350>

You can make your life easier by providing a list of values for the x-axis (e.g., `x=['No', 'Yes']`)
"""

data.info()

data['By_River'] = np.where(data.CHAS == 0, 'No', 'Yes')
river_counts = data.By_River.value_counts()
river_counts

display(river_counts.index)
display(river_counts.values)

bar = px.bar(x=river_counts.index,
             y=river_counts.values,
             color=river_counts.index,
             title='Next to Charles River?',
             opacity=0.7
             )
bar.update_layout(xaxis_title='By The River?',
                   yaxis_title='Number of Properties',)
 
bar.show()

"""<img src=https://i.imgur.com/b5UaBal.jpg height=350>"""

# Her Way:
# First, count values ()
river_access = data['CHAS'].value_counts()
display(river_access)

# X-axis is purely descriptive, so doesn't have to be from table
# Lot of typing to change the color!
bar = px.bar(x=['No', 'Yes'],
             y=river_access.values,
             color=river_access.values,
             color_continuous_scale=px.colors.sequential.haline,
             title='Next to Charles River?')

bar.update_layout(xaxis_title='Property Located Next to the River?', 
                  yaxis_title='Number of Homes',
                  coloraxis_showscale=False)
bar.show()

"""# Understand the Relationships in the Data

### Run a Pair Plot

**Challenge**

There might be some relationships in the data that we should know about. Before you run the code, make some predictions:

* What would you expect the relationship to be between pollution (NOX) and the distance to employment (DIS)? 
* What kind of relationship do you expect between the number of rooms (RM) and the home value (PRICE)?
* What about the amount of poverty in an area (LSTAT) and home prices? 

Run a [Seaborn `.pairplot()`](https://seaborn.pydata.org/generated/seaborn.pairplot.html?highlight=pairplot#seaborn.pairplot) to visualise all the relationships at the same time. Note, this is a big task and can take 1-2 minutes! After it's finished check your intuition regarding the questions above on the `pairplot`.
"""

data.sample(1)

# Super useful!!! Shows EVERY relationship graphed, with a histogram for graphs of itself.

# UNCOMMENT NEXT LINE TO RUN GRAPH.      --CURRENTLY COMMENTED OUT SINCE IT SLOWS DOWN RUNTIME CONSIDERABLY

#sns.pairplot(data)

# You can even include a regression line
# sns.pairplot(data, kind='reg', plot_kws={'line_kws':{'color': 'cyan'}})
plt.show()

"""**Challenge**

Use [Seaborn's `.jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) to look at some of the relationships in more detail. Create a jointplot for:

* DIS and NOX
* INDUS vs NOX
* LSTAT vs RM
* LSTAT vs PRICE
* RM vs PRICE

Try adding some opacity or `alpha` to the scatter plots using keyword arguments under `joint_kws`.

#### Distance from Employment vs. Pollution

**Challenge**: 

Compare DIS (Distance from employment) with NOX (Nitric Oxide Pollution) using Seaborn's `.jointplot()`. Does pollution go up or down as the distance increases?
"""

# Changing dot opacity helps to show overlapping points
sns.jointplot(x=data.DIS,
              y=data.NOX,
              joint_kws={'alpha':0.5},
              height=8)

"""#### Proportion of Non-Retail Industry üè≠üè≠üè≠ versus Pollution 

**Challenge**: 

Compare INDUS (the proportion of non-retail industry i.e., factories) with NOX (Nitric Oxide Pollution) using Seaborn's `.jointplot()`. Does pollution go up or down as there is a higher proportion of industry?
"""

sns.jointplot(x=data.INDUS,
              y=data.NOX,
              joint_kws={'alpha':0.5},
              height=8,
              color='orange' 
              
              )

"""#### % of Lower Income Population vs Average Number of Rooms

**Challenge** 

Compare LSTAT (proportion of lower-income population) with RM (number of rooms) using Seaborn's `.jointplot()`. How does the number of rooms per dwelling vary with the poverty of area? Do homes have more or fewer rooms when LSTAT is low?
"""

sns.jointplot(x=data.LSTAT,
              y=data.RM,
              joint_kws={'alpha':0.5},
              height=8,
              color='deeppink')

"""#### % of Lower Income Population versus Home Price

**Challenge**

Compare LSTAT with PRICE using Seaborn's `.jointplot()`. How does the proportion of the lower-income population in an area affect home prices?
"""

sns.jointplot(x=data.LSTAT,
              y=data.PRICE,
              joint_kws={'alpha':0.5},
              height=8,
              color='crimson')

"""#### Number of Rooms versus Home Value

**Challenge** 

Compare RM (number of rooms) with PRICE using Seaborn's `.jointplot()`. You can probably guess how the number of rooms affects home prices. üòä 
"""

sns.jointplot(x=data.RM,
              y=data.PRICE,
              joint_kws={'alpha':0.5},
              height=8,
              color='darkblue')

"""# Split Training & Test Dataset

We *can't* use all 506 entries in our dataset to train our model. The reason is that we want to evaluate our model on data that it hasn't seen yet (i.e., out-of-sample data). That way we can get a better idea of its performance in the real world. 

**Challenge**

* Import the [`train_test_split()` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn
* Create 4 subsets: X_train, X_test, y_train, y_test
* Split the training and testing data roughly 80/20. 
* To get the same random split every time you run your notebook use `random_state=10`. This helps us get the same results every time and avoid confusion while we're learning. 


Hint: Remember, your **target** is your home PRICE, and your **features** are all the other columns you'll use to predict the price. 

"""

from sklearn import model_selection
from sklearn.model_selection import train_test_split

# Shows column names, easier than typing them all out in the next step
data.columns

X = pd.DataFrame(data, columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 
                                'TAX','PTRATIO', 'B', 'LSTAT'])
y = pd.DataFrame(data, columns=['PRICE'])

# Output is for data sets, so we assign 4 variables
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)

display(X_train, X_test, y_train, y_test)

# Her Way
# More elegant solution to the column issue
# axis=0 is the index, axis=1 is the column headers, axis=2 is ?

target = data['PRICE']
features = data.drop(['PRICE', 'By_River'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(features, 
                                                    target, 
                                                    test_size=0.2, 
                                                    random_state=10)

# Her Way (so extra)
# % of training set
train_pct = 100*len(X_train)/len(features)
print(f'Training data is {train_pct:.3}% of the total data.')

# % of test data set
test_pct = 100*X_test.shape[0]/features.shape[0]
print(f'Test data makes up the remaining {test_pct:0.3}%.')

"""# Multivariable Regression

In a previous lesson, we had a linear model with only a single feature (our movie budgets). This time we have a total of 13 features. Therefore, our Linear Regression model will have the following form:

$$ PR \hat ICE = \theta _0 + \theta _1 RM + \theta _2 NOX + \theta _3 DIS + \theta _4 CHAS ... + \theta _{13} LSTAT$$

### Run Your First Regression

**Challenge**

Use sklearn to run the regression on the training dataset. How high is the r-squared for the regression on the training data?
"""

# Create a regression line to use below
regression = LinearRegression()

# Pass through our training sets to build our model.
# .score shows the r_squared value

regression.fit(X_train, y_train)
display(regression.score(X_train, y_train))

"""### Evaluate the Coefficients of the Model

Here we do a sense check on our regression coefficients. The first thing to look for is if the coefficients have the expected sign (positive or negative). 

**Challenge** Print out the coefficients (the thetas in the equation above) for the features. Hint: You'll see a nice table if you stick the coefficients in a DataFrame. 

* We already saw that RM on its own had a positive relation to PRICE based on the scatter plot. Is RM's coefficient also positive?
* What is the sign on the LSAT coefficient? Does it match your intuition and the scatter plot above?
* Check the other coefficients. Do they have the expected sign?
* Based on the coefficients, how much more expensive is a room with 6 rooms compared to a room with 5 rooms? According to the model, what is the premium you would have to pay for an extra room? 
"""

reg_coef = pd.DataFrame(regression.coef_)
reg_coef

reg_coef.rename(columns={0:'CRIM', 1:'ZN', 2:'INDUS', 3:'CHAS', 4:'NOX', 5:'RM', 6:'AGE', 7:'DIS', 8:'RAD', 
                                9:'TAX', 10:'PTRATIO', 11:'B', 12:'LSTAT'}, inplace=True)

reg_coef

#Her Way:
# Reformat everything from the start, awesome!
regr_coef = pd.DataFrame(data=regression.coef_, index=X_train.columns, columns=['Coefficient'])
regr_coef

# Premium for having an extra room
premium = regr_coef.loc['RM'].values[0] * 1000  # i.e., 3.11 * 1000
print(f'The price premium for having an extra room is ${premium:.5}')

"""### Analyse the Estimated Values & Regression Residuals

The next step is to evaluate our regression. How good our regression is depends not only on the r-squared. It also depends on the **residuals** - the difference between the model's predictions ($\hat y_i$) and the true values ($y_i$) inside `y_train`. 

```
predicted_values = regr.predict(X_train)
residuals = (y_train - predicted_values)
```

**Challenge**: Create two scatter plots.

The first plot should be actual values (`y_train`) against the predicted value values: 

<img src=https://i.imgur.com/YMttBNV.png height=350>

The cyan line in the middle shows `y_train` against `y_train`. If the predictions had been 100% accurate then all the dots would be on this line. The further away the dots are from the line, the worse the prediction was. That makes the distance to the cyan line, you guessed it, our residuals üòä


The second plot should be the residuals against the predicted prices. Here's what we're looking for: 

<img src=https://i.imgur.com/HphsBsj.png height=350>


"""

# .predict runs our training x set, returns the calculated y price values
predicted_values = regression.predict(X_train)

# Note: With matplotlib, the normal .plot doesn't take the keyword arg 'x=' or 'y=', just put them in

plt.figure(figsize=(10,6), dpi=200)
plt.scatter(x=y_train,
            y=predicted_values,
            alpha=0.7, 
            color='purple',
            )

plt.plot(y_train,
         y_train,
         color='cyan',
         linewidth=2
          )

plt.title("Actual vs Predicted House Prices", fontsize=17)
plt.xlabel("\nActual Prices (y_train)", fontsize=18)
plt.ylabel("Predicted Prices\n", fontsize=18)
plt.show()

# basically the distance from the line; the difference in actual nad predicted
residuals = (y_train - predicted_values)
zero_resid = (y_train - y_train)

plt.figure(dpi=100)
plt.scatter(x=predicted_values, 
            y=residuals,
            alpha=0.7, 
            color='indigo'
            )

plt.plot(predicted_values,
         zero_resid,
         color='cyan',
         linewidth=2
          )

plt.title('Actual vs Predicted House Prices', fontsize=18)
plt.xlabel("\nPredicted Prices (y_train)", fontsize=18)
plt.ylabel("Model Residuals\n", fontsize=18)
plt.show()

"""Why do we want to look at the residuals? We want to check that they look random. Why? The residuals represent the errors of our model. If there's a pattern in our errors, then our model has a systematic bias.

We can analyse the distribution of the residuals. In particular, we're interested in the **skew** and the **mean**.

In an ideal case, what we want is something close to a normal distribution. A normal distribution has a skewness of 0 and a mean of 0. A skew of 0 means that the distribution is symmetrical - the bell curve is not lopsided or biased to one side. Here's what a normal distribution looks like: 

<img src=https://i.imgur.com/7QBqDtO.png height=400>

**Challenge**

* Calculate the mean and the skewness of the residuals. 
* Again, use Seaborn's `.displot()` to create a histogram and superimpose the Kernel Density Estimate (KDE)
* Is the skewness different from zero? If so, by how much? 
* Is the mean different from zero?
"""

residuals

with sns.axes_style("darkgrid"):
  ax1 = sns.displot(residuals,
                    fill=True,
                    kde=True,
                    aspect=2)
plt.xlim(-15, 25)
plt.show()

residuals.mean()

display(residuals.skew())
print('\nApparently, 1.46 skew is not good, hence the next text statement from her')
print('A higher skew in the model\'s residuals means that there could be systemic bias in our model')

# Her Way
# Notice the different shape: she doesn't have 'aspect=2' like I do above
# Residual Distribution Chart
resid_mean = round(residuals.mean(), 2)
resid_skew = round(residuals.skew(), 2)

sns.displot(residuals, kde=True, color='indigo')
plt.title(f'Residuals Skew ({resid_skew}) Mean ({resid_mean})')
plt.show()

"""### Data Transformations for a Better Fit

We have two options at this point: 

1. Change our model entirely. Perhaps a linear model is not appropriate. 
2. Transform our data to make it fit better with our linear model. 

Let's try a data transformation approach. 

**Challenge**

Investigate if the target `data['PRICE']` could be a suitable candidate for a log transformation. 

* Use Seaborn's `.displot()` to show a histogram and KDE of the price data. 
* Calculate the skew of that distribution.
* Use [NumPy's `log()` function](https://numpy.org/doc/stable/reference/generated/numpy.log.html) to create a Series that has the log prices
* Plot the log prices using Seaborn's `.displot()` and calculate the skew. 
* Which distribution has a skew that's closer to zero? 

"""

# Now we investigate the skew of Price column from our original data set vs a log of the price column
price_skew = data.PRICE.skew()
sns.displot(data, x='PRICE', kde=True)
# plt.xlim(-15, 25)
plt.title(f'Normal Prices: Skew = {price_skew:.3}')
plt.show()

data.PRICE.skew()

data['LOG_PRICE'] = np.log(data.PRICE)

log_skew = data.LOG_PRICE.skew()
with sns.axes_style("darkgrid"):
  ax1 = sns.displot(data,
                    x='LOG_PRICE',
                    fill=True,
                    kde=True,
                    color='darkgreen')
plt.title(f'Log Prices: Skew = {log_skew:.3}')
# plt.xlim(-15, 25)
plt.show()

# Skew is much closer to 0, so its a better fit
data.LOG_PRICE.skew()

"""#### How does the log transformation work?

Using a log transformation does not affect every price equally. Large prices are affected more than smaller prices in the dataset. Here's how the prices are "compressed" by the log transformation:

<img src=https://i.imgur.com/TH8sK1Q.png height=200>

We can see this when we plot the actual prices against the (transformed) log prices. 
"""

plt.figure(dpi=150)
plt.scatter(data.PRICE, np.log(data.PRICE))

plt.title('Mapping the Original Price to a Log Price')
plt.ylabel('Log Price')
plt.xlabel('Actual $ Price in 1000s')
plt.show()

"""## Regression using Log Prices

Using log prices instead, our model has changed to:

$$ \log (PR \hat ICE) = \theta _0 + \theta _1 RM + \theta _2 NOX + \theta_3 DIS + \theta _4 CHAS + ... + \theta _{13} LSTAT $$

**Challenge**: 

* Use `train_test_split()` with the same random state as before to make the results comparable. 
* Run a second regression, but this time use the transformed target data. 
* What is the r-squared of the regression on the training data? 
* Have we improved the fit of our model compared to before based on this measure?

"""

X_log = pd.DataFrame(data, columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 
                                'TAX','PTRATIO', 'B', 'LSTAT'])
y_log = pd.DataFrame(data, columns=['LOG_PRICE'])

X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_log, y_log, test_size=0.2, random_state=10)

# display(X_train_log, X_test_log, y_train_log, y_test_log)

reg_log = LinearRegression()

reg_log.fit(X_train_log, y_train_log)

# Slightly higher r-squared than the normal price (.75 vs .79)
reg_log.score(X_train_log, y_train_log)

"""##OVERVIEW OF TRAIN_TEST_SPLIT()

"""

# Her Way:
# Notice she created a new variable of the df rather than changing the df like I did
new_target = np.log(data['PRICE']) # Use log prices
features = data.drop(['PRICE','By_River'], axis=1)

log_X_train, log_X_test, log_y_train, log_y_test = train_test_split(features, 
                                                    new_target, 
                                                    test_size=0.2, 
                                                    random_state=10)

log_regr = LinearRegression()
log_regr.fit(log_X_train, log_y_train)
log_rsquared = log_regr.score(log_X_train, log_y_train)

log_predictions = log_regr.predict(log_X_train)
log_residuals = (log_y_train - log_predictions)

print(f'Training data r-squared: {log_rsquared:.2}')

"""## Evaluating Coefficients with Log Prices

**Challenge**: Print out the coefficients of the new regression model. 

* Do the coefficients still have the expected sign? 
* Is being next to the river a positive based on the data?
* How does the quality of the schools affect property prices? What happens to prices as there are more students per teacher? 

Hint: Use a DataFrame to make the output look pretty. 
"""

log_coef = pd.DataFrame(reg_log.coef_)
log_coef

log_coef.rename(columns={0:'CRIM', 1:'ZN', 2:'INDUS', 3:'CHAS', 4:'NOX', 5:'RM', 6:'AGE', 7:'DIS', 8:'RAD', 
                                9:'TAX', 10:'PTRATIO', 11:'B', 12:'LSTAT'}, inplace=True)

log_coef

"""## Regression with Log Prices & Residual Plots

**Challenge**: 

* Copy-paste the cell where you've created scatter plots of the actual versus the predicted home prices as well as the residuals versus the predicted values. 
* Add 2 more plots to the cell so that you can compare the regression outcomes with the log prices side by side. 
* Use `indigo` as the colour for the original regression and `navy` for the color using log prices.
"""

predicted_values_log = reg_log.predict(X_train_log)

plt.figure(figsize=(10,6), dpi=200)
plt.scatter(x=y_train_log,
            y=predicted_values_log,
            alpha=0.7, 
            color='purple',
            )

plt.plot(y_train_log,
         y_train_log,
         color='cyan',
         linewidth=2
          )

# plt.title("Actual vs Predicted House Prices")
plt.xlabel("\nActual Prices - Log Scale (y_train)", fontsize=18)
plt.ylabel("Predicted Prices\n", fontsize=18)
plt.show()

residuals_log = (y_train_log - predicted_values_log)

# Remember, we want randomness in distribution. Patterns mean systemic bias in our model 
plt.figure(figsize=(10,6), dpi=200)
plt.scatter(x=predicted_values_log, 
            y=residuals_log,
            alpha=0.7, 
            color='purple',
            )

# plt.title='Actual vs Predicted House Prices'
plt.xlabel("\nPredicted Prices - Log Scale (y_train)", fontsize=18)
plt.ylabel("Model Residuals\n", fontsize=18)
plt.show()

"""**Challenge**: 

Calculate the mean and the skew for the residuals using log prices. Are the mean and skew closer to 0 for the regression using log prices?
"""

resid_log_mean = residuals_log.mean()

# Woah! way closer to 0
resid_log_skew = residuals_log.skew()
display(resid_log_skew)

sns.displot(residuals_log,
                    x='LOG_PRICE',
                    fill=True,
                    kde=True,
                    color='navy'
                    )
plt.xlim(-0.8, 0.8)
plt.title(f'Log price model: Residuals Skew ({resid_log_skew.values[0]:.3}) Mean ({round(resid_log_mean.values[0],2)})')
plt.show()

# Almost perfectly 'normal distribution' going on for our log residuals

#Her Way:
# Distribution of Residuals (log prices) --- checking for normality
log_resid_mean = round(residuals_log.mean(), 2)
log_resid_skew = round(residuals_log.skew(), 2)

sns.displot(residuals_log, kde=True, color='navy')

plt.show()

sns.displot(residuals, kde=True, color='indigo')
plt.title(f'Original model: Residuals Skew ({log_resid_skew}) Mean ({log_resid_mean})')
plt.show()

"""# Compare Out of Sample Performance

The *real* test is how our model performs on data that it has not "seen" yet. This is where our `X_test` comes in. 

**Challenge**

Compare the r-squared of the two models on the test dataset. Which model does better? Is the r-squared higher or lower than for the training dataset? Why?
"""

# Slightly higher r-squared than the normal price (.75 vs .79)
# This is with the data we used to make the model.
display(regression.score(X_train, y_train))
display(reg_log.score(X_train_log, y_train_log))

# Now we use the NEW data (test) that the model hasn't seen before
# Lower values are expected, result is pretty good.
display(regression.score(X_test, y_test))
display(reg_log.score(X_test_log, y_test_log))

"""# Predict a Property's Value using the Regression Coefficients

Our preferred model now has an equation that looks like this:

$$ \log (PR \hat ICE) = \theta _0 + \theta _1 RM + \theta _2 NOX + \theta_3 DIS + \theta _4 CHAS + ... + \theta _{13} LSTAT $$

The average property has the mean value for all its charactistics:
"""

# Starting Point: Average Values in the Dataset
# Gotta get rid of the extra boolean row we created earlier for river since we're going to average values
features = data.drop(['PRICE','By_River'], axis=1)
display(features)
# Calculates values, turns it into an array.
average_vals = features.mean().values
display(average_vals)
# Array table isn't pretty and doesn't have column headings
# Transform back in df with all the columns from before. Array is shape (14,1), needs to be (1,14). Reshape!
property_stats = pd.DataFrame(data=average_vals.reshape(1, len(features.columns)), 
                              columns=features.columns)
property_stats

# Her Way
# WAAYYYY SIMPLER! Use .predict for given inputs!
stats = property_stats.drop(['LOG_PRICE'], axis=1)
log_estimate = reg_log.predict(stats)[0][0]
print(f'The log price estimate is ${log_estimate:.3}')

# Convert Log Prices to Acutal Dollar Values
dollar_est = np.e**log_estimate * 1000
# or use
dollar_est = np.exp(log_estimate) * 1000
print(f'The property is estimated to be worth ${dollar_est:.6}')

"""**Challenge**

Predict how much the average property is worth using the stats above. What is the log price estimate and what is the dollar estimate? You'll have to [reverse the log transformation with `.exp()`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html?highlight=exp#numpy.exp) to find the dollar value. 
"""

# My way, the very manual method of putting the variable values into the equation.
# Again, just use .predict for faster results
house_vars = property_stats.drop(['LOG_PRICE'], axis=1)
coefs = pd.DataFrame(reg_log.coef_)
intercept = reg_log.intercept_
house_vars

display(coefs)
coefs.rename(columns={0:'CRIM', 1:'ZN', 2:'INDUS', 3:'CHAS', 4:'NOX', 5:'RM', 6:'AGE', 7:'DIS', 8:'RAD', 
                                9:'TAX', 10:'PTRATIO', 11:'B', 12:'LSTAT'}, inplace=True)
coefs

variables = coefs.values * house_vars.values
display(variables)
est_log_price = intercept + variables.sum()
print(f'\nModel estimated Avg Log Price = {est_log_price[0]:.6}')
print('This is also exactly what we see from the property_stats above, we did it!')

# Converts log price back into normal dollar price:
dollar_price = np.exp(est_log_price)
print(f'\nModel estimated Avg Dollar Price = ${dollar_price[0]:.4}k')

"""**Challenge**

Keeping the average values for CRIM, RAD, INDUS and others, value a property with the following characteristics:
"""

# Define Property Characteristics
next_to_river = True
nr_rooms = 8
students_per_classroom = 20 
distance_to_town = 5
pollution = data.NOX.quantile(q=0.75) # high
amount_of_poverty =  data.LSTAT.quantile(q=0.25) # low

house_params = {
                'CHAS': int(next_to_river == True),
                'RM':nr_rooms,
                'PTRATIO':students_per_classroom,
                'DIS':distance_to_town,
                'NOX':pollution,
                'LSTAT':amount_of_poverty
                }

# df.at[0,'age']=40 ---- finds exact cell and changes value: df[index, column]=new_value 

for key in house_params:
  house_vars.at[0,key]=house_params[key]
house_vars

model_log_price_est = intercept + (coefs.values * house_vars.values).sum()
print(f'\nModel estimated property log_price = ${model_log_price_est[0]:.3}')
dollar_price = np.exp(model_log_price_est)
print(f'\nModel estimated property price = ${(dollar_price[0]*1000):.6}')

# Done! Got pretty much exactly what she did the whole way through on my own(just less elegantly)